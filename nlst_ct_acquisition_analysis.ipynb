{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLST CT Acquisition Parameter Analysis\n",
    "\n",
    "This notebook analyzes DICOM CT acquisition parameter distributions from the NLST (National Lung Screening Trial) collection in IDC to identify a representative subset that covers the variety of acquisitions.\n",
    "\n",
    "**Data Source:** `bigquery-public-data.idc_current.dicom_all`\n",
    "\n",
    "**Key DICOM attributes analyzed:**\n",
    "- Spatial Resolution: SliceThickness, PixelSpacing, SpacingBetweenSlices\n",
    "- Contrast/Exposure: KVP, Exposure, CTDIvol\n",
    "- Reconstruction: ConvolutionKernel\n",
    "- Hardware: Manufacturer, ManufacturerModelName\n",
    "- Geometry: PatientPosition, GantryDetectorTilt, SpiralPitchFactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & BigQuery Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed (uncomment to run)\n",
    "# %pip install --upgrade google-cloud-bigquery pandas matplotlib seaborn scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.cloud import bigquery\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'project_id': None,  # Set to your GCP project ID, or None for default credentials\n",
    "    'collection_id': 'nlst',\n",
    "    'table': 'bigquery-public-data.idc_current.dicom_all',\n",
    "    'modality': 'CT',\n",
    "    'n_representatives_per_cluster': 10,  # Number of representative series per cluster\n",
    "}\n",
    "\n",
    "# Key DICOM attributes to analyze\n",
    "DICOM_ATTRIBUTES = {\n",
    "    'spatial': ['SliceThickness', 'PixelSpacing', 'SpacingBetweenSlices'],\n",
    "    'exposure': ['KVP', 'Exposure', 'CTDIvol'],\n",
    "    'reconstruction': ['ConvolutionKernel'],\n",
    "    'hardware': ['Manufacturer', 'ManufacturerModelName'],\n",
    "    'geometry': ['PatientPosition', 'GantryDetectorTilt', 'SpiralPitchFactor'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BigQuery client\n",
    "def get_bq_client(project_id=None):\n",
    "    \"\"\"Initialize BigQuery client with optional project ID.\"\"\"\n",
    "    if project_id:\n",
    "        return bigquery.Client(project=project_id)\n",
    "    return bigquery.Client()\n",
    "\n",
    "client = get_bq_client(CONFIG['project_id'])\n",
    "print(f\"Connected to BigQuery. Project: {client.project}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def run_query(query, client=client):\n",
    "    \"\"\"Execute BigQuery query and return DataFrame.\"\"\"\n",
    "    job = client.query(query)\n",
    "    return job.to_dataframe()\n",
    "\n",
    "def estimate_query_cost(query, client=client):\n",
    "    \"\"\"Estimate query cost before execution (dry run).\"\"\"\n",
    "    job_config = bigquery.QueryJobConfig(dry_run=True)\n",
    "    job = client.query(query, job_config=job_config)\n",
    "    bytes_processed = job.total_bytes_processed\n",
    "    gb_processed = bytes_processed / 1e9\n",
    "    estimated_cost = gb_processed * 5 / 1000  # $5 per TB\n",
    "    print(f\"Query will scan {gb_processed:.2f} GB (estimated cost: ${estimated_cost:.4f})\")\n",
    "    return bytes_processed\n",
    "\n",
    "def safe_float(col):\n",
    "    \"\"\"SQL expression to safely cast column to FLOAT64.\"\"\"\n",
    "    return f\"SAFE_CAST({col} AS FLOAT64)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2.1: Count records in NLST CT collection\n",
    "query_counts = f\"\"\"\n",
    "SELECT\n",
    "  COUNT(*) as total_instances,\n",
    "  COUNT(DISTINCT PatientID) as unique_patients,\n",
    "  COUNT(DISTINCT StudyInstanceUID) as unique_studies,\n",
    "  COUNT(DISTINCT SeriesInstanceUID) as unique_series\n",
    "FROM `{CONFIG['table']}`\n",
    "WHERE collection_id = '{CONFIG['collection_id']}'\n",
    "  AND Modality = '{CONFIG['modality']}'\n",
    "\"\"\"\n",
    "\n",
    "estimate_query_cost(query_counts)\n",
    "counts_df = run_query(query_counts)\n",
    "print(\"\\nNLST CT Collection Summary:\")\n",
    "print(counts_df.T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hierarchy counts\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "levels = ['Patients', 'Studies', 'Series', 'Instances']\n",
    "values = [counts_df['unique_patients'].iloc[0], \n",
    "          counts_df['unique_studies'].iloc[0],\n",
    "          counts_df['unique_series'].iloc[0], \n",
    "          counts_df['total_instances'].iloc[0]]\n",
    "\n",
    "bars = ax.bar(levels, values, color=['#0072B2', '#009E73', '#D55E00', '#CC79A7'])\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('NLST CT Collection: DICOM Hierarchy Counts')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{val:,}', \n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2.2: Check attribute completeness\n",
    "all_attributes = (DICOM_ATTRIBUTES['spatial'] + DICOM_ATTRIBUTES['exposure'] + \n",
    "                  DICOM_ATTRIBUTES['reconstruction'] + DICOM_ATTRIBUTES['hardware'] + \n",
    "                  DICOM_ATTRIBUTES['geometry'])\n",
    "\n",
    "completeness_queries = []\n",
    "for attr in all_attributes:\n",
    "    completeness_queries.append(\n",
    "        f\"SELECT '{attr}' as attribute, COUNT({attr}) as non_null_count, \"\n",
    "        f\"COUNT(*) - COUNT({attr}) as null_count \"\n",
    "        f\"FROM `{CONFIG['table']}` \"\n",
    "        f\"WHERE collection_id = '{CONFIG['collection_id']}' AND Modality = '{CONFIG['modality']}'\"\n",
    "    )\n",
    "\n",
    "query_completeness = \" UNION ALL \".join(completeness_queries)\n",
    "completeness_df = run_query(query_completeness)\n",
    "completeness_df['completeness_pct'] = (completeness_df['non_null_count'] / \n",
    "                                        (completeness_df['non_null_count'] + completeness_df['null_count']) * 100)\n",
    "print(\"Attribute Completeness:\")\n",
    "print(completeness_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attribute completeness\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "colors = ['#2ecc71' if pct > 90 else '#f39c12' if pct > 50 else '#e74c3c' \n",
    "          for pct in completeness_df['completeness_pct']]\n",
    "\n",
    "bars = ax.barh(completeness_df['attribute'], completeness_df['completeness_pct'], color=colors)\n",
    "ax.set_xlabel('Completeness (%)')\n",
    "ax.set_title('NLST CT Attribute Completeness')\n",
    "ax.set_xlim(0, 105)\n",
    "\n",
    "# Add percentage labels\n",
    "for bar, pct in zip(bars, completeness_df['completeness_pct']):\n",
    "    ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, f'{pct:.1f}%', \n",
    "            ha='left', va='center', fontsize=9)\n",
    "\n",
    "ax.axvline(x=90, color='green', linestyle='--', alpha=0.5, label='90% threshold')\n",
    "ax.axvline(x=50, color='orange', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2.3: Series-level summary (main data extraction)\n",
    "query_series = f\"\"\"\n",
    "SELECT\n",
    "  SeriesInstanceUID,\n",
    "  PatientID,\n",
    "  StudyInstanceUID,\n",
    "  Manufacturer,\n",
    "  ManufacturerModelName,\n",
    "  {safe_float('SliceThickness')} as SliceThickness,\n",
    "  {safe_float('SpacingBetweenSlices')} as SpacingBetweenSlices,\n",
    "  SAFE_CAST(SPLIT(PixelSpacing, '\\\\\\\\')[OFFSET(0)] AS FLOAT64) as PixelSpacing_Row,\n",
    "  {safe_float('KVP')} as KVP,\n",
    "  {safe_float('Exposure')} as Exposure,\n",
    "  {safe_float('CTDIvol')} as CTDIvol,\n",
    "  ConvolutionKernel,\n",
    "  PatientPosition,\n",
    "  {safe_float('GantryDetectorTilt')} as GantryDetectorTilt,\n",
    "  {safe_float('SpiralPitchFactor')} as SpiralPitchFactor,\n",
    "  COUNT(*) as instance_count\n",
    "FROM `{CONFIG['table']}`\n",
    "WHERE collection_id = '{CONFIG['collection_id']}'\n",
    "  AND Modality = '{CONFIG['modality']}'\n",
    "GROUP BY\n",
    "  SeriesInstanceUID, PatientID, StudyInstanceUID,\n",
    "  Manufacturer, ManufacturerModelName,\n",
    "  SliceThickness, SpacingBetweenSlices, PixelSpacing,\n",
    "  KVP, Exposure, CTDIvol,\n",
    "  ConvolutionKernel, PatientPosition, GantryDetectorTilt, SpiralPitchFactor\n",
    "\"\"\"\n",
    "\n",
    "print(\"Fetching series-level data...\")\n",
    "estimate_query_cost(query_series)\n",
    "series_df = run_query(query_series)\n",
    "print(f\"\\nRetrieved {len(series_df):,} series\")\n",
    "print(f\"\\nDataFrame shape: {series_df.shape}\")\n",
    "print(f\"\\nColumn types:\\n{series_df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "series_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Distribution Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Spatial Resolution Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial resolution distributions\n",
    "spatial_cols = ['SliceThickness', 'PixelSpacing_Row', 'SpacingBetweenSlices']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, col in enumerate(spatial_cols):\n",
    "    ax = axes[i]\n",
    "    data = series_df[col].dropna()\n",
    "    \n",
    "    ax.hist(data, bins=50, edgecolor='black', alpha=0.7, color='#0072B2')\n",
    "    ax.set_xlabel(f'{col} (mm)')\n",
    "    ax.set_ylabel('Series Count')\n",
    "    ax.set_title(f'{col}\\n(n={len(data):,}, median={data.median():.2f})')\n",
    "    \n",
    "    # Add statistics\n",
    "    stats_text = f'Mean: {data.mean():.2f}\\nStd: {data.std():.2f}\\nMin: {data.min():.2f}\\nMax: {data.max():.2f}'\n",
    "    ax.text(0.95, 0.95, stats_text, transform=ax.transAxes, \n",
    "            verticalalignment='top', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5), fontsize=8)\n",
    "\n",
    "plt.suptitle('Spatial Resolution Parameter Distributions', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial parameters by manufacturer\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, col in enumerate(spatial_cols):\n",
    "    ax = axes[i]\n",
    "    # Filter to manufacturers with sufficient data\n",
    "    mfr_counts = series_df['Manufacturer'].value_counts()\n",
    "    valid_mfrs = mfr_counts[mfr_counts > 100].index\n",
    "    plot_df = series_df[series_df['Manufacturer'].isin(valid_mfrs)]\n",
    "    \n",
    "    sns.boxplot(data=plot_df, x='Manufacturer', y=col, ax=ax, palette='Set2')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax.set_title(f'{col} by Manufacturer')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Contrast/Exposure Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KVP distribution (typically discrete values)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# KVP bar chart\n",
    "ax1 = axes[0]\n",
    "kvp_counts = series_df['KVP'].value_counts().sort_index()\n",
    "ax1.bar(kvp_counts.index.astype(str), kvp_counts.values, color='#E69F00', edgecolor='black')\n",
    "ax1.set_xlabel('KVP')\n",
    "ax1.set_ylabel('Series Count')\n",
    "ax1.set_title(f'KVP Distribution (n={series_df[\"KVP\"].notna().sum():,})')\n",
    "\n",
    "# Exposure histogram\n",
    "ax2 = axes[1]\n",
    "exposure_data = series_df['Exposure'].dropna()\n",
    "ax2.hist(exposure_data, bins=50, edgecolor='black', alpha=0.7, color='#009E73')\n",
    "ax2.set_xlabel('Exposure (mAs)')\n",
    "ax2.set_ylabel('Series Count')\n",
    "ax2.set_title(f'Exposure Distribution\\n(n={len(exposure_data):,}, median={exposure_data.median():.1f})')\n",
    "\n",
    "# CTDIvol histogram\n",
    "ax3 = axes[2]\n",
    "ctdi_data = series_df['CTDIvol'].dropna()\n",
    "if len(ctdi_data) > 0:\n",
    "    ax3.hist(ctdi_data, bins=50, edgecolor='black', alpha=0.7, color='#D55E00')\n",
    "    ax3.set_xlabel('CTDIvol (mGy)')\n",
    "    ax3.set_ylabel('Series Count')\n",
    "    ax3.set_title(f'CTDIvol Distribution\\n(n={len(ctdi_data):,}, median={ctdi_data.median():.1f})')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'CTDIvol not available', ha='center', va='center', transform=ax3.transAxes)\n",
    "    ax3.set_title('CTDIvol Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exposure vs CTDIvol correlation (if both available)\n",
    "if series_df['CTDIvol'].notna().sum() > 100 and series_df['Exposure'].notna().sum() > 100:\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    valid_data = series_df[['Exposure', 'CTDIvol']].dropna()\n",
    "    ax.scatter(valid_data['Exposure'], valid_data['CTDIvol'], alpha=0.3, s=10, c='#0072B2')\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr, p_val = stats.spearmanr(valid_data['Exposure'], valid_data['CTDIvol'])\n",
    "    ax.set_xlabel('Exposure (mAs)')\n",
    "    ax.set_ylabel('CTDIvol (mGy)')\n",
    "    ax.set_title(f'Exposure vs CTDIvol\\n(Spearman r = {corr:.3f}, p < 0.001)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient data for Exposure vs CTDIvol comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Reconstruction Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution kernel distribution\n",
    "kernel_counts = series_df['ConvolutionKernel'].value_counts()\n",
    "print(f\"Total unique convolution kernels: {len(kernel_counts)}\")\n",
    "print(f\"\\nTop 20 kernels:\")\n",
    "print(kernel_counts.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 kernels bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "top_kernels = kernel_counts.head(20)\n",
    "colors = plt.cm.viridis(np.linspace(0, 0.8, len(top_kernels)))\n",
    "ax.barh(range(len(top_kernels)), top_kernels.values, color=colors)\n",
    "ax.set_yticks(range(len(top_kernels)))\n",
    "ax.set_yticklabels(top_kernels.index)\n",
    "ax.set_xlabel('Series Count')\n",
    "ax.set_title('Top 20 Convolution Kernels')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernels by manufacturer (stacked)\n",
    "kernel_mfr = series_df.groupby(['ConvolutionKernel', 'Manufacturer']).size().unstack(fill_value=0)\n",
    "top_kernel_mfr = kernel_mfr.loc[kernel_counts.head(15).index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "top_kernel_mfr.plot(kind='barh', stacked=True, ax=ax, colormap='Set2')\n",
    "ax.set_xlabel('Series Count')\n",
    "ax.set_title('Top 15 Convolution Kernels by Manufacturer')\n",
    "ax.legend(title='Manufacturer', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Hardware (Manufacturer & Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manufacturer distribution\n",
    "mfr_counts = series_df['Manufacturer'].value_counts()\n",
    "print(\"Manufacturer Distribution:\")\n",
    "print(mfr_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manufacturer pie chart and model bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Pie chart\n",
    "ax1 = axes[0]\n",
    "colors = ['#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2', '#D55E00', '#CC79A7']\n",
    "wedges, texts, autotexts = ax1.pie(\n",
    "    mfr_counts.values, \n",
    "    labels=mfr_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    colors=colors[:len(mfr_counts)],\n",
    "    explode=[0.02] * len(mfr_counts)\n",
    ")\n",
    "ax1.set_title('Series Distribution by Manufacturer')\n",
    "\n",
    "# Top models bar chart\n",
    "ax2 = axes[1]\n",
    "model_counts = series_df.groupby(['Manufacturer', 'ManufacturerModelName']).size().sort_values(ascending=False)\n",
    "top_models = model_counts.head(10)\n",
    "model_labels = [f\"{m[0]}\\n{m[1]}\" if m[1] else m[0] for m in top_models.index]\n",
    "\n",
    "ax2.barh(range(len(top_models)), top_models.values, color='#0072B2')\n",
    "ax2.set_yticks(range(len(top_models)))\n",
    "ax2.set_yticklabels(model_labels, fontsize=8)\n",
    "ax2.set_xlabel('Series Count')\n",
    "ax2.set_title('Top 10 Scanner Models')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Geometry Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geometry parameters\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Patient Position\n",
    "ax1 = axes[0]\n",
    "pos_counts = series_df['PatientPosition'].value_counts()\n",
    "ax1.bar(pos_counts.index.astype(str), pos_counts.values, color='#56B4E9', edgecolor='black')\n",
    "ax1.set_xlabel('Patient Position')\n",
    "ax1.set_ylabel('Series Count')\n",
    "ax1.set_title('Patient Position Distribution')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gantry Detector Tilt\n",
    "ax2 = axes[1]\n",
    "tilt_data = series_df['GantryDetectorTilt'].dropna()\n",
    "if len(tilt_data) > 0:\n",
    "    ax2.hist(tilt_data, bins=50, edgecolor='black', alpha=0.7, color='#009E73')\n",
    "    ax2.set_xlabel('Gantry Tilt (degrees)')\n",
    "    ax2.set_ylabel('Series Count')\n",
    "    ax2.set_title(f'Gantry Tilt Distribution\\n(n={len(tilt_data):,})')\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax2.transAxes)\n",
    "\n",
    "# Spiral Pitch Factor\n",
    "ax3 = axes[2]\n",
    "pitch_data = series_df['SpiralPitchFactor'].dropna()\n",
    "if len(pitch_data) > 0:\n",
    "    ax3.hist(pitch_data, bins=50, edgecolor='black', alpha=0.7, color='#D55E00')\n",
    "    ax3.set_xlabel('Spiral Pitch Factor')\n",
    "    ax3.set_ylabel('Series Count')\n",
    "    ax3.set_title(f'Spiral Pitch Distribution\\n(n={len(pitch_data):,})')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax3.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Statistical Assumption Checking & Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Distribution Analysis (Before Statistical Tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define continuous variables for analysis\n",
    "continuous_cols = ['SliceThickness', 'PixelSpacing_Row', 'SpacingBetweenSlices', \n",
    "                   'KVP', 'Exposure', 'CTDIvol', 'GantryDetectorTilt', 'SpiralPitchFactor']\n",
    "\n",
    "# Filter to columns with sufficient data\n",
    "continuous_cols_valid = [col for col in continuous_cols \n",
    "                         if col in series_df.columns and series_df[col].notna().sum() > 100]\n",
    "print(f\"Continuous variables with sufficient data: {continuous_cols_valid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality tests\n",
    "def test_normality(data, name, max_samples=5000):\n",
    "    \"\"\"Test normality using D'Agostino-Pearson test (robust for large samples).\"\"\"\n",
    "    data = data.dropna()\n",
    "    if len(data) > max_samples:\n",
    "        data = data.sample(n=max_samples, random_state=42)\n",
    "    \n",
    "    if len(data) < 20:\n",
    "        return {'variable': name, 'n': len(data), 'statistic': np.nan, \n",
    "                'p_value': np.nan, 'is_normal': 'Insufficient data'}\n",
    "    \n",
    "    try:\n",
    "        stat, p_value = stats.normaltest(data)\n",
    "        is_normal = 'Yes' if p_value > 0.05 else 'No'\n",
    "    except:\n",
    "        stat, p_value, is_normal = np.nan, np.nan, 'Test failed'\n",
    "    \n",
    "    return {'variable': name, 'n': len(data), 'statistic': stat, \n",
    "            'p_value': p_value, 'is_normal': is_normal}\n",
    "\n",
    "normality_results = [test_normality(series_df[col], col) for col in continuous_cols_valid]\n",
    "normality_df = pd.DataFrame(normality_results)\n",
    "print(\"Normality Test Results (D'Agostino-Pearson):\")\n",
    "print(normality_df.to_string(index=False))\n",
    "print(\"\\nNote: With large samples, even small deviations from normality are significant.\")\n",
    "print(\"Consider visual inspection (Q-Q plots) and practical significance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Q plots for visual normality assessment\n",
    "n_cols = len(continuous_cols_valid)\n",
    "n_rows = (n_cols + 2) // 3\n",
    "fig, axes = plt.subplots(n_rows, 3, figsize=(12, 3*n_rows))\n",
    "axes = axes.flatten() if n_cols > 1 else [axes]\n",
    "\n",
    "for i, col in enumerate(continuous_cols_valid):\n",
    "    ax = axes[i]\n",
    "    data = series_df[col].dropna()\n",
    "    if len(data) > 5000:\n",
    "        data = data.sample(n=5000, random_state=42)\n",
    "    \n",
    "    stats.probplot(data, dist=\"norm\", plot=ax)\n",
    "    ax.set_title(f'Q-Q Plot: {col}')\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(len(continuous_cols_valid), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.suptitle('Q-Q Plots for Normality Assessment', y=1.02, fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homoscedasticity test (Levene's test) for key variables across manufacturers\n",
    "def test_homoscedasticity(df, var, group_var='Manufacturer'):\n",
    "    \"\"\"Test variance equality across groups using Levene's test.\"\"\"\n",
    "    groups = [group[var].dropna().values for name, group in df.groupby(group_var)]\n",
    "    groups = [g for g in groups if len(g) > 1]\n",
    "    \n",
    "    if len(groups) < 2:\n",
    "        return {'variable': var, 'statistic': np.nan, 'p_value': np.nan, 'equal_variance': 'Insufficient groups'}\n",
    "    \n",
    "    try:\n",
    "        stat, p_value = stats.levene(*groups)\n",
    "        equal_var = 'Yes' if p_value > 0.05 else 'No'\n",
    "    except:\n",
    "        stat, p_value, equal_var = np.nan, np.nan, 'Test failed'\n",
    "    \n",
    "    return {'variable': var, 'statistic': stat, 'p_value': p_value, 'equal_variance': equal_var}\n",
    "\n",
    "homoscedasticity_results = [test_homoscedasticity(series_df, col) for col in continuous_cols_valid]\n",
    "homoscedasticity_df = pd.DataFrame(homoscedasticity_results)\n",
    "print(\"Homoscedasticity Test Results (Levene's Test):\")\n",
    "print(homoscedasticity_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: Recommended statistical tests based on assumptions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICAL TEST RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nBased on the normality and homoscedasticity tests:\")\n",
    "print(\"\\n1. CORRELATIONS:\")\n",
    "print(\"   - Most variables are non-normal → Use Spearman correlation\")\n",
    "print(\"\\n2. GROUP COMPARISONS (by Manufacturer):\")\n",
    "print(\"   - Non-normal distributions and unequal variances → Use Kruskal-Wallis\")\n",
    "print(\"\\n3. CATEGORICAL ASSOCIATIONS:\")\n",
    "print(\"   - Check expected cell counts before Chi-square test\")\n",
    "print(\"   - If expected counts < 5 in >20% cells → Combine categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman correlation matrix (robust to non-normality)\n",
    "corr_data = series_df[continuous_cols_valid].dropna()\n",
    "if len(corr_data) > 0:\n",
    "    corr_matrix = corr_data.corr(method='spearman')\n",
    "    \n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        corr_matrix, \n",
    "        mask=mask,\n",
    "        annot=True, \n",
    "        fmt='.2f',\n",
    "        cmap='RdBu_r',\n",
    "        center=0,\n",
    "        vmin=-1, vmax=1,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title('Spearman Correlation Matrix: CT Acquisition Parameters', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient data for correlation analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot (subsample for performance)\n",
    "pairplot_cols = ['SliceThickness', 'PixelSpacing_Row', 'KVP', 'Exposure']\n",
    "pairplot_cols = [c for c in pairplot_cols if c in continuous_cols_valid]\n",
    "\n",
    "if len(pairplot_cols) >= 2:\n",
    "    # Subsample and filter to main manufacturers\n",
    "    top_mfrs = series_df['Manufacturer'].value_counts().head(3).index\n",
    "    pairplot_data = series_df[series_df['Manufacturer'].isin(top_mfrs)][pairplot_cols + ['Manufacturer']].dropna()\n",
    "    \n",
    "    if len(pairplot_data) > 3000:\n",
    "        pairplot_data = pairplot_data.sample(n=3000, random_state=42)\n",
    "    \n",
    "    g = sns.pairplot(\n",
    "        pairplot_data,\n",
    "        hue='Manufacturer',\n",
    "        diag_kind='kde',\n",
    "        plot_kws={'alpha': 0.5, 's': 10},\n",
    "        palette='Set2'\n",
    "    )\n",
    "    g.fig.suptitle('Parameter Relationships by Manufacturer', y=1.02)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient variables for pair plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Group Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kruskal-Wallis test for continuous parameters across manufacturers\n",
    "def kruskal_wallis_by_group(df, continuous_var, group_var='Manufacturer'):\n",
    "    \"\"\"Perform Kruskal-Wallis H-test for group comparisons.\"\"\"\n",
    "    groups = [group[continuous_var].dropna().values for name, group in df.groupby(group_var)]\n",
    "    groups = [g for g in groups if len(g) > 1]\n",
    "    \n",
    "    if len(groups) < 2:\n",
    "        return {'variable': continuous_var, 'H_statistic': np.nan, 'p_value': np.nan, 'n_groups': len(groups)}\n",
    "    \n",
    "    try:\n",
    "        h_stat, p_value = stats.kruskal(*groups)\n",
    "        \n",
    "        # Calculate effect size (epsilon-squared)\n",
    "        n_total = sum(len(g) for g in groups)\n",
    "        epsilon_sq = (h_stat - len(groups) + 1) / (n_total - len(groups))\n",
    "        epsilon_sq = max(0, epsilon_sq)  # Ensure non-negative\n",
    "        \n",
    "    except:\n",
    "        h_stat, p_value, epsilon_sq = np.nan, np.nan, np.nan\n",
    "    \n",
    "    return {'variable': continuous_var, 'H_statistic': h_stat, 'p_value': p_value, \n",
    "            'epsilon_squared': epsilon_sq, 'n_groups': len(groups)}\n",
    "\n",
    "kruskal_results = [kruskal_wallis_by_group(series_df, col) for col in continuous_cols_valid]\n",
    "kruskal_df = pd.DataFrame(kruskal_results)\n",
    "print(\"Kruskal-Wallis Test Results (by Manufacturer):\")\n",
    "print(kruskal_df.to_string(index=False))\n",
    "print(\"\\nEffect size interpretation (epsilon-squared):\")\n",
    "print(\"  Small: < 0.01, Medium: 0.01-0.06, Large: > 0.14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square test for categorical associations (Manufacturer vs ConvolutionKernel)\n",
    "def chi_square_with_check(df, var1, var2, min_expected=5):\n",
    "    \"\"\"Perform chi-square test with expected count validation.\"\"\"\n",
    "    contingency = pd.crosstab(df[var1].fillna('Missing'), df[var2].fillna('Missing'))\n",
    "    \n",
    "    chi2, p_value, dof, expected = stats.chi2_contingency(contingency)\n",
    "    \n",
    "    # Check expected counts\n",
    "    pct_low_expected = (expected < min_expected).sum() / expected.size * 100\n",
    "    \n",
    "    # Calculate Cramer's V\n",
    "    n = contingency.sum().sum()\n",
    "    min_dim = min(contingency.shape) - 1\n",
    "    cramers_v = np.sqrt(chi2 / (n * min_dim)) if min_dim > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'chi2': chi2,\n",
    "        'p_value': p_value,\n",
    "        'dof': dof,\n",
    "        'cramers_v': cramers_v,\n",
    "        'pct_low_expected': pct_low_expected,\n",
    "        'valid_test': pct_low_expected < 20\n",
    "    }\n",
    "\n",
    "# Test Manufacturer vs ConvolutionKernel (top categories only)\n",
    "top_kernels = series_df['ConvolutionKernel'].value_counts().head(10).index\n",
    "filtered_df = series_df[series_df['ConvolutionKernel'].isin(top_kernels)]\n",
    "\n",
    "chi2_result = chi_square_with_check(filtered_df, 'Manufacturer', 'ConvolutionKernel')\n",
    "print(\"Chi-Square Test: Manufacturer vs ConvolutionKernel (top 10 kernels)\")\n",
    "print(f\"  Chi-square statistic: {chi2_result['chi2']:.2f}\")\n",
    "print(f\"  p-value: {chi2_result['p_value']:.2e}\")\n",
    "print(f\"  Degrees of freedom: {chi2_result['dof']}\")\n",
    "print(f\"  Cramer's V (effect size): {chi2_result['cramers_v']:.3f}\")\n",
    "print(f\"  % cells with expected count < 5: {chi2_result['pct_low_expected']:.1f}%\")\n",
    "print(f\"  Test valid: {chi2_result['valid_test']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Clustering for Representative Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for clustering\n",
    "def prepare_clustering_features(df, numeric_cols, categorical_cols, top_n_categories=10):\n",
    "    \"\"\"Prepare feature matrix for clustering.\"\"\"\n",
    "    # Start with a copy\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Filter to rows with sufficient numeric data\n",
    "    valid_numeric = [col for col in numeric_cols if col in df_clean.columns]\n",
    "    df_clean = df_clean.dropna(subset=valid_numeric, how='all')\n",
    "    \n",
    "    # Fill remaining NaN with median\n",
    "    for col in valid_numeric:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "    \n",
    "    # Encode categorical variables (top N + 'Other')\n",
    "    for col in categorical_cols:\n",
    "        if col in df_clean.columns:\n",
    "            top_cats = df_clean[col].value_counts().nlargest(top_n_categories).index\n",
    "            df_clean[f'{col}_encoded'] = df_clean[col].apply(\n",
    "                lambda x: x if x in top_cats else 'Other'\n",
    "            )\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X_numeric = df_clean[valid_numeric]\n",
    "    \n",
    "    cat_encoded_cols = [f'{c}_encoded' for c in categorical_cols if f'{c}_encoded' in df_clean.columns]\n",
    "    X_cat = pd.get_dummies(df_clean[cat_encoded_cols], drop_first=True)\n",
    "    \n",
    "    X = pd.concat([X_numeric, X_cat], axis=1)\n",
    "    \n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled, X.columns.tolist(), df_clean, scaler\n",
    "\n",
    "# Define features\n",
    "numeric_features = ['SliceThickness', 'PixelSpacing_Row', 'KVP', 'Exposure']\n",
    "numeric_features = [f for f in numeric_features if f in series_df.columns and series_df[f].notna().sum() > 100]\n",
    "\n",
    "categorical_features = ['Manufacturer', 'ConvolutionKernel']\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "X_scaled, feature_names, df_clean, scaler = prepare_clustering_features(\n",
    "    series_df, numeric_features, categorical_features\n",
    ")\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X_scaled.shape}\")\n",
    "print(f\"Total features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of clusters\n",
    "def find_optimal_clusters(X, k_range=range(2, 15)):\n",
    "    \"\"\"Determine optimal number of clusters using multiple metrics.\"\"\"\n",
    "    inertias = []\n",
    "    silhouettes = []\n",
    "    calinski = []\n",
    "    \n",
    "    # Subsample for faster computation if needed\n",
    "    if len(X) > 10000:\n",
    "        np.random.seed(42)\n",
    "        idx = np.random.choice(len(X), 10000, replace=False)\n",
    "        X_sample = X[idx]\n",
    "    else:\n",
    "        X_sample = X\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(X_sample)\n",
    "        \n",
    "        inertias.append(kmeans.inertia_)\n",
    "        silhouettes.append(silhouette_score(X_sample, labels))\n",
    "        calinski.append(calinski_harabasz_score(X_sample, labels))\n",
    "    \n",
    "    # Plot metrics\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].plot(k_range, inertias, 'bo-')\n",
    "    axes[0].set_xlabel('Number of Clusters (K)')\n",
    "    axes[0].set_ylabel('Inertia')\n",
    "    axes[0].set_title('Elbow Method')\n",
    "    \n",
    "    axes[1].plot(k_range, silhouettes, 'go-')\n",
    "    axes[1].set_xlabel('Number of Clusters (K)')\n",
    "    axes[1].set_ylabel('Silhouette Score')\n",
    "    axes[1].set_title('Silhouette Analysis')\n",
    "    \n",
    "    axes[2].plot(k_range, calinski, 'ro-')\n",
    "    axes[2].set_xlabel('Number of Clusters (K)')\n",
    "    axes[2].set_ylabel('Calinski-Harabasz Score')\n",
    "    axes[2].set_title('Calinski-Harabasz Index')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Recommend K based on silhouette score\n",
    "    best_k = list(k_range)[np.argmax(silhouettes)]\n",
    "    print(f\"Recommended K based on silhouette score: {best_k}\")\n",
    "    \n",
    "    return {'k_range': list(k_range), 'silhouettes': silhouettes, 'best_k': best_k}\n",
    "\n",
    "cluster_metrics = find_optimal_clusters(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User can override the recommended K\n",
    "n_clusters = cluster_metrics['best_k']  # Or set manually: n_clusters = 8\n",
    "print(f\"Using {n_clusters} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df_clean['Cluster'] = labels\n",
    "\n",
    "print(f\"Cluster sizes:\")\n",
    "print(df_clean['Cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# PCA scatter plot\n",
    "scatter = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels, \n",
    "                           cmap='viridis', alpha=0.5, s=10)\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "axes[0].set_title('Cluster Assignments (PCA Projection)')\n",
    "plt.colorbar(scatter, ax=axes[0], label='Cluster')\n",
    "\n",
    "# Cluster sizes bar chart\n",
    "cluster_counts = df_clean['Cluster'].value_counts().sort_index()\n",
    "axes[1].bar(cluster_counts.index, cluster_counts.values, color='#0072B2')\n",
    "axes[1].set_xlabel('Cluster')\n",
    "axes[1].set_ylabel('Number of Series')\n",
    "axes[1].set_title('Cluster Sizes')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterize clusters\n",
    "def characterize_clusters(df, numeric_cols, categorical_cols):\n",
    "    \"\"\"Generate summary statistics for each cluster.\"\"\"\n",
    "    # Numeric summary\n",
    "    numeric_summary = df.groupby('Cluster')[numeric_cols].agg(['mean', 'std', 'median'])\n",
    "    \n",
    "    # Categorical summary (mode)\n",
    "    cat_summary = {}\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            mode_df = df.groupby('Cluster')[col].agg(\n",
    "                lambda x: x.value_counts().index[0] if len(x) > 0 and x.notna().any() else None\n",
    "            )\n",
    "            cat_summary[col] = mode_df\n",
    "    \n",
    "    return numeric_summary, pd.DataFrame(cat_summary)\n",
    "\n",
    "numeric_summary, cat_summary = characterize_clusters(df_clean, numeric_features, categorical_features)\n",
    "\n",
    "print(\"Cluster Characterization - Numeric Features (Mean):\")\n",
    "print(numeric_summary.xs('mean', axis=1, level=1).round(2).to_string())\n",
    "print(\"\\nCluster Characterization - Categorical Features (Mode):\")\n",
    "print(cat_summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select representative series from each cluster\n",
    "def select_representatives(X, labels, df_original, n_per_cluster=10):\n",
    "    \"\"\"Select series closest to cluster centroids.\"\"\"\n",
    "    representatives = []\n",
    "    \n",
    "    for cluster_id in range(labels.max() + 1):\n",
    "        # Get cluster members\n",
    "        cluster_mask = labels == cluster_id\n",
    "        cluster_X = X[cluster_mask]\n",
    "        cluster_df = df_original[cluster_mask].reset_index(drop=True)\n",
    "        \n",
    "        if len(cluster_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate centroid\n",
    "        centroid = cluster_X.mean(axis=0)\n",
    "        \n",
    "        # Calculate distances to centroid\n",
    "        distances = np.linalg.norm(cluster_X - centroid, axis=1)\n",
    "        \n",
    "        # Select closest series\n",
    "        n_select = min(n_per_cluster, len(cluster_df))\n",
    "        closest_indices = distances.argsort()[:n_select]\n",
    "        \n",
    "        for idx in closest_indices:\n",
    "            rep_series = cluster_df.iloc[idx].to_dict()\n",
    "            rep_series['Distance_to_Centroid'] = distances[idx]\n",
    "            representatives.append(rep_series)\n",
    "    \n",
    "    return pd.DataFrame(representatives)\n",
    "\n",
    "n_per_cluster = CONFIG['n_representatives_per_cluster']\n",
    "representatives_df = select_representatives(X_scaled, labels, df_clean, n_per_cluster)\n",
    "\n",
    "print(f\"Selected {len(representatives_df)} representative series\")\n",
    "print(f\"\\nRepresentatives per cluster:\")\n",
    "print(representatives_df['Cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview representatives\n",
    "display_cols = ['SeriesInstanceUID', 'Cluster', 'Manufacturer', 'ConvolutionKernel', \n",
    "                'SliceThickness', 'KVP', 'Distance_to_Centroid']\n",
    "display_cols = [c for c in display_cols if c in representatives_df.columns]\n",
    "representatives_df[display_cols].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = './nlst_analysis_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Export cluster assignments (all series)\n",
    "cluster_export_cols = ['SeriesInstanceUID', 'PatientID', 'Cluster', 'Manufacturer', \n",
    "                       'ConvolutionKernel'] + numeric_features\n",
    "cluster_export_cols = [c for c in cluster_export_cols if c in df_clean.columns]\n",
    "df_clean[cluster_export_cols].to_csv(f'{output_dir}/all_series_clusters.csv', index=False)\n",
    "print(f\"Saved: {output_dir}/all_series_clusters.csv\")\n",
    "\n",
    "# Export representative series\n",
    "representatives_df.to_csv(f'{output_dir}/representative_series.csv', index=False)\n",
    "print(f\"Saved: {output_dir}/representative_series.csv\")\n",
    "\n",
    "# Export Series UIDs only (for download)\n",
    "uid_list = representatives_df['SeriesInstanceUID'].tolist()\n",
    "with open(f'{output_dir}/representative_series_uids.txt', 'w') as f:\n",
    "    f.write('\\n'.join(uid_list))\n",
    "print(f\"Saved: {output_dir}/representative_series_uids.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate download script for idc-index\n",
    "download_script = f'''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Download representative NLST CT series using idc-index.\n",
    "Generated by NLST CT Acquisition Analysis notebook.\n",
    "\"\"\"\n",
    "\n",
    "from idc_index import IDCClient\n",
    "\n",
    "# Initialize IDC client (no authentication required for downloads)\n",
    "client = IDCClient()\n",
    "\n",
    "# List of representative SeriesInstanceUIDs\n",
    "series_uids = {repr(uid_list)}\n",
    "\n",
    "# Download series\n",
    "print(f\"Downloading {{len(series_uids)}} representative series...\")\n",
    "client.download_from_selection(\n",
    "    seriesInstanceUID=series_uids,\n",
    "    downloadDir=\"./nlst_representative_series\",\n",
    "    dirTemplate=\"%collection_id/%PatientID/%SeriesInstanceUID\"\n",
    ")\n",
    "print(\"Download complete!\")\n",
    "'''\n",
    "\n",
    "with open(f'{output_dir}/download_representatives.py', 'w') as f:\n",
    "    f.write(download_script)\n",
    "print(f\"Saved: {output_dir}/download_representatives.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal series analyzed: {len(df_clean):,}\")\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Representatives selected: {len(representatives_df)}\")\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  - {output_dir}/all_series_clusters.csv\")\n",
    "print(f\"  - {output_dir}/representative_series.csv\")\n",
    "print(f\"  - {output_dir}/representative_series_uids.txt\")\n",
    "print(f\"  - {output_dir}/download_representatives.py\")\n",
    "print(\"\\nTo download the representative series, run:\")\n",
    "print(f\"  python {output_dir}/download_representatives.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
